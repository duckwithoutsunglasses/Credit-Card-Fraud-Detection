# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1peu1mlGbSMRT2JByqc9CexqNHvZV_-Ll
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score, recall_score

df = pd.read_csv('creditcard.csv')

df['Class'].value_counts().rename({0:'Not Fraud', 1:'Fraud'})

"""# **Neural Networks**"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv1D, MaxPool1D
from tensorflow.keras.optimizers import Adam

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

df['Class'].value_counts()

"""Balance Dataset"""

not_fraud = df[df['Class']==0]
fraud = df[df['Class']==1]

not_fraud.shape

fraud.shape

# Makes the shape on not fraud and fraud the same

not_fraud = not_fraud.sample(fraud.shape[0])
not_fraud.shape

# Combines the reshaped non_fraud and fraud into one dataframe
df = fraud.append(not_fraud, ignore_index = True)
df

df['Class'].value_counts()

# X and y for train test split
X = df.drop('Class', axis = 1)
y = df['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)

X_train.shape

X_test.shape

X_train

# Standardize X_train/test data (also turns it into numpy arrays)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
# No fit_transform prevents overfitting
X_test = scaler.transform(X_test)

X_train

X_test

# Changed to numpy array
y_train = y_train.to_numpy()

y_test = y_test.to_numpy()

"""Reshape parameters to build CNN"""

X_train.shape

# Makes it 3d
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

X_train.shape, X_test.shape

"""Build CNN"""

epochs  = 20
model = Sequential()
model.add(Conv1D(32, 2, activation='relu', input_shape = X_train[0].shape))
model.add(BatchNormalization())
model.add(Dropout(0.2)) # 20% dropped after

model.add(Conv1D(64, 2, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(1, activation='sigmoid')) # Sigmoid for binary classification

model.summary()

model.compile(optimizer=Adam(lr=0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test,  y_test), verbose = 1)

"""Plotting learning curve"""

def plot_learning_curve(history, epoch):
  epoch_range = range(1, epoch+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc = 'upper left')
  plt.show()

  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc = 'upper left')
  plt.show()

plot_learning_curve(history, epochs)

"""Adding MaxPool"""

epochs  = 50
model = Sequential()
model.add(Conv1D(32, 2, activation='relu', input_shape = X_train[0].shape))
model.add(BatchNormalization())
model.add(MaxPool1D(2)) ### Copy and pasted everything but this line
model.add(Dropout(0.2)) # 20% dropped after

model.add(Conv1D(64, 2, activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool1D(2)) ### Copy and pasted everything but this line
model.add(Dropout(0.5)) # 50% dropped after

model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(1, activation='sigmoid')) # Sigmoid for binary classification

model.compile(optimizer=Adam(lr=0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test,  y_test), verbose = 1)
plot_learning_curve(history, epochs)